{"cells":[{"cell_type":"markdown","metadata":{"id":"nhBvBbp09E-a"},"source":["## Include file path"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":232,"status":"ok","timestamp":1638579852389,"user":{"displayName":"Chenchen Huang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11152487919426388388"},"user_tz":480},"id":"RIrVmIwocBHX","outputId":"4ecb1ad7-2c20-4ad3-8f9a-f07e8c4dde48"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import sys\n","sys.path.append('/content/drive/Shareddrives/AME508_Project/lib')\n","sys.path.append('/content/drive/Shareddrives/AME508_Project/data')"]},{"cell_type":"markdown","metadata":{"id":"iBSW8fXK9Ly3"},"source":["## import package"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6163,"status":"ok","timestamp":1638579260953,"user":{"displayName":"Chenchen Huang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11152487919426388388"},"user_tz":480},"id":"SOLh8UjofN35","outputId":"293f46bb-e326-4252-b4f0-2b8ffeade7c2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found GPU at: /device:GPU:0\n"]}],"source":["import numpy as np\n","import tensorflow as tf\n","import scipy\n","import scipy.io\n","\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))\n","\n","from tensorflow import keras\n","%matplotlib inline \n","import matplotlib\n","import matplotlib.pyplot as plt\n","from tensorflow.keras import datasets, layers, models\n","from keras.regularizers import l2"]},{"cell_type":"markdown","metadata":{"id":"3fIhe2F1Cp2R"},"source":["## load traning data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mX7MB6Gc1m1a"},"outputs":[],"source":["import h5py\n","\n","with h5py.File('/content/drive/Shareddrives/AME508_Project/data/train_val_data_two/train_dataset_two.mat', 'r') as file:\n","    train_data = list(file['data_train'])\n","\n","with h5py.File('/content/drive/Shareddrives/AME508_Project/data/train_val_data_two/val_dataset_two.mat', 'r') as file:\n","    val_data = list(file['data_val'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"APCwKpjlUOgm"},"outputs":[],"source":["train_y = train_data[4]\n","\n","train_x1 = np.zeros((len(train_y),2))\n","train_x1[:,0] = train_data[2]\n","train_x1[:,1] = train_data[3]\n","# train_x1 = np.reshape(train_x1,(len(train_y),1))\n","\n","train_x2 = np.zeros((len(train_x1),2))\n","train_x2[:,0] = train_data[0]\n","train_x2[:,1] = train_data[1]\n","\n","val_y = val_data[4]\n","val_x1 = np.zeros((len(val_y),2))\n","val_x1[:,0] = val_data[2]\n","val_x1[:,1] = val_data[3]\n","val_x2 = np.zeros((len(val_x1),2))\n","val_x2[:,0] = val_data[0]\n","val_x2[:,1] = val_data[1]\n"]},{"cell_type":"markdown","metadata":{"id":"eiYlowVaCvTm"},"source":["## define Neural network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HC8xWrpXBdeO"},"outputs":[],"source":["class MyModel(tf.keras.Model):\n","\n","  def __init__(self,input_dim_1,input_dim_2,dot_dim,width,depth,reg_param=1e-6):\n","    super(MyModel, self).__init__()\n","    Reg_Func = l2\n","    self.depth=depth\n","    self.layers1=[]\n","    self.layers2=[]\n","\n","    for i  in range(self.depth-1):\n","      bias_ini=keras.initializers.RandomUniform(minval=-1, maxval=1, seed=None)\n","      kernel_ini = keras.initializers.RandomUniform(minval=-1, maxval=1, seed=None)\n","      self.layers1.append(tf.keras.layers.Dense(width, activation=tf.nn.tanh, kernel_initializer=kernel_ini, bias_initializer=bias_ini,kernel_regularizer=Reg_Func(reg_param)))\n","      self.layers2.append(tf.keras.layers.Dense(width, activation=tf.nn.tanh, kernel_initializer=kernel_ini, bias_initializer=bias_ini,kernel_regularizer=Reg_Func(reg_param)))\n","    self.layers1.append(tf.keras.layers.Dense(dot_dim, activation=tf.nn.tanh, kernel_initializer=kernel_ini, bias_initializer=bias_ini,kernel_regularizer=Reg_Func(reg_param)))\n","    self.layers2.append(tf.keras.layers.Dense(dot_dim, activation=tf.nn.tanh, kernel_initializer=kernel_ini, bias_initializer=bias_ini,kernel_regularizer=Reg_Func(reg_param)))\n","\n","\n","  def call(self, inputs_1, inputs_2):\n","    for i in range(self.depth):\n","      layer1=self.layers1[i]\n","      layer2=self.layers2[i]\n","      inputs_1=layer1(inputs_1)\n","      inputs_2=layer2(inputs_2)\n","    return tf.reduce_sum( tf.multiply( inputs_1, inputs_2 ), 1 )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3w_VF9bIBnX2"},"outputs":[],"source":["def train():\n","\n","  tf.keras.backend.clear_session()\n","  input_dim_1=2\n","  input_dim_2=2\n","  dot_dim=30\n","  width=15\n","  depth=15\n","  reg_param=1e-6\n","  learning_rate=1e-3\n","  model=MyModel(input_dim_1=input_dim_1,input_dim_2=input_dim_2,dot_dim=dot_dim,width=width,depth=depth,reg_param=reg_param)\n","  \n","  optimizer   = keras.optimizers.Adam(learning_rate=learning_rate) \n","  max_epoch   = 1000000\n","  batch_size  = 64\n","\n","  los=[]\n","  val_los = []\n","  for epoch in range(1,max_epoch+1):\n","      # print(epoch)\n","      train_batch_index=np.random.choice(len(train_y),batch_size)\n","      val_batch_index=np.random.choice(len(val_y),batch_size)\n","      with tf.GradientTape() as tape1:\n","        out=model(train_x1[train_batch_index,:],train_x2[train_batch_index,:])\n","        val_out = model(val_x1[val_batch_index,:],val_x2[val_batch_index,:])\n","        # model.summary()\n","        # print(tf.shape(out))\n","        loss=tf.reduce_mean(tf.square(out-train_y[train_batch_index]))\n","        val_loss = tf.reduce_mean(tf.square(val_out-val_y[val_batch_index]))\n","      los.append(loss.numpy())\n","      val_los.append(val_loss.numpy())\n","\n","      grads = tape1.gradient(loss, model.trainable_variables)\n","      \n","      optimizer.apply_gradients(zip(grads, model.trainable_variables)) # zip used to create an iterator over the tuples\n","      # print(loss)\n","      if epoch % 100 == 0 or epoch==max_epoch:\n","          print(f\"Epoch: {epoch}, loss: {loss:.2e}, val_loss: {val_loss:.2e}\")\n","      if epoch % 5000 == 0  or epoch==max_epoch:\n","          model.save_weights('/content/drive/Shareddrives/AME508_Project/code/trained_model/model/model_{}'.format(epoch))\n","      # if epoch % 100 == 0:\n","      #     print(grads)\n","  return  model,los#,loss_int,loss_bc"]},{"cell_type":"markdown","metadata":{"id":"Kn2B2ULhC05z"},"source":["## training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vc8YnROcb2FM"},"outputs":[],"source":["model,loss=train()"]},{"cell_type":"markdown","metadata":{"id":"Q_1g_NsdC2gK"},"source":["## load Neutral Network"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1566,"status":"ok","timestamp":1638579534250,"user":{"displayName":"Chenchen Huang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"11152487919426388388"},"user_tz":480},"id":"LeZu43GR4pU4","outputId":"5c92f476-fd03-4238-f865-14e00a69f38c"},"outputs":[{"data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f3d6095a8d0>"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["input_dim_1=2\n","input_dim_2=2\n","dot_dim=30\n","width=15\n","depth=15\n","reg_param=1e-6\n","learning_rate=1e-3\n","\n","model=MyModel(input_dim_1=input_dim_1,input_dim_2=input_dim_2,dot_dim=dot_dim,width=width,depth=depth,reg_param=reg_param)\n","\n","file_name = '/content/drive/Shareddrives/AME508_Project/code/trained_model/model/model_1000000'\n","model.load_weights(file_name)\n"]},{"cell_type":"markdown","metadata":{"id":"sZ9sNP4NC7KH"},"source":["## optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_uIUykKSx_si"},"outputs":[],"source":["import random\n","import scipy.io\n","import h5py\n","import numpy as np\n","\n","file=scipy.io.loadmat('/content/drive/Shareddrives/AME508_Project/data/observation/14.mat')\n","known_data = file['u']\n","\n","with h5py.File('/content/drive/Shareddrives/AME508_Project/data/observation/grid.mat', 'r') as file:\n","    x = list(file['x'])\n","    t = list(file['t'])\n","\n","x_t = np.zeros((1000*1000,2))\n","x = np.reshape(x,(1000*1000,))\n","t = np.reshape(t,(1000*1000,))\n","x_t[:,0] = x;\n","x_t[:,1] = t;\n","\n","\n","# a = np.zeros((2,))\n","# a[0] = random.uniform(1e-4, 1e-1)\n","# a[1] = random.uniform(1,10)\n","a = tf.Variable([[0.05,5]], name='a', trainable=True, dtype=tf.float32)\n","optimizer   = keras.optimizers.Adam(1e-2) \n","max_epoch = 500\n","for epoch in range(1,max_epoch+1):\n","\n","  with tf.GradientTape() as tape:\n","\n","    predict_output = model.call(a,x_t)\n","\n","    # predict_output = output.numpy()\n","\n","    loss = tf.reduce_mean(tf.square(predict_output-known_data))\n","\n","  trainable_variables = [a]\n","  grads = tape.gradient(loss, trainable_variables)\n","\n","  optimizer.apply_gradients(zip(grads, trainable_variables))\n","  if epoch % 10 == 0 or epoch==max_epoch:\n","    print(f\"Epoch: {epoch}, a: {a.numpy()}, loss: {loss:.2e}\")"]},{"cell_type":"markdown","metadata":{"id":"U5TR5ZA9CKWT"},"source":["## tfco (tensorflow constrained optimization)\n","(experiment)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0hGIRgiHCI-P"},"outputs":[],"source":["#######################\n","# import tensorflow_constrained_optimization as tfco\n","\n","# class optProblem(tfco.ConstrainedMinimizationProblem):\n","#   def __init__(self, loss_fn,var,known_data,x_t):\n","#     self._loss_fn = loss_fn\n","#     self._var = var\n","#     self._known_data = known_data;\n","#     self._x_t = x_t;\n","#   @property\n","#   def num_constraints(self):\n","#     return 4\n","  \n","#   def objective(self):\n","#     return loss_fn(self._x_t,self._known_data)\n","\n","#   def constraints(self):\n","#     a1, a2 = self._var\n","#     l1 = 1e-4 - a1\n","#     r1 = a1-1e-1\n","#     l2 = 1-a2\n","#     r2 = a2 - 10\n","#     constraints = tf.stack([l1,r1,l2,r2])\n","#     return constraints\n","\n","# def loss_fn(x_t,known_data):\n","\n","#   predict_output = model.call(tf.reshape(tf.stack([a1,a2]),[1,2]),x_t)\n","#   loss = tf.reduce_mean(tf.square(predict_output-known_data))\n","#   return loss\n","\n","# # a = tf.Variable([[0.1,1]], name='a', trainable=True, dtype=tf.float32)\n","# a1 = tf.Variable(0.05, dtype=tf.float32, name='a1')\n","# a2 = tf.Variable(5.0, dtype=tf.float32, name='a2')\n","\n","# file=scipy.io.loadmat('/Users/huangchenchen/Dropbox/ame508_testing/measured_data_0.0017_3.3333.mat')\n","# known_data = file['u']\n","# with h5py.File('/Users/huangchenchen/Dropbox/ame508_testing/grid.mat', 'r') as file:\n","#     x = list(file['x'])\n","#     t = list(file['t'])\n","# x_t = np.zeros((1000*1000,2))\n","# x = np.reshape(x,(1000*1000,))\n","# t = np.reshape(t,(1000*1000,))\n","# x_t[:,0] = x;\n","# x_t[:,1] = t;\n","\n","# problem = optProblem(loss_fn, [a1,a2], known_data,x_t)\n","\n","# optimizer = tfco.LagrangianOptimizerV2(\n","#     optimizer=tf.optimizers.Adagrad(learning_rate=1e-2),\n","#     num_constraints=problem.num_constraints\n","# )\n","# var_list = [a1,a2] + list(problem.trainable_variables) + optimizer.trainable_variables()\n","\n","# for i in range(10000):\n","#     optimizer.minimize(problem, var_list=var_list)\n","#     if i % 10 == 0:\n","#         # print(f'step = {i}')\n","#         # print(f'loss = {loss_fn(x_t,known_data)}')\n","#         print(f\"step = {i}, a1: {a1.numpy()}, a2: {a2.numpy()}, loss = {loss_fn(x_t,known_data):.2e}\")\n","\n","###########################3\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"main_AME508.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
